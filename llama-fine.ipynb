{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport torch\ndevice ='cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-24T15:41:33.512133Z","iopub.execute_input":"2024-10-24T15:41:33.512977Z","iopub.status.idle":"2024-10-24T15:41:37.195577Z","shell.execute_reply.started":"2024-10-24T15:41:33.512930Z","shell.execute_reply":"2024-10-24T15:41:37.194445Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install transformers accelerate torch huggingface_hub peft","metadata":{"execution":{"iopub.status.busy":"2024-10-24T15:41:40.650414Z","iopub.execute_input":"2024-10-24T15:41:40.651185Z","iopub.status.idle":"2024-10-24T15:41:53.679984Z","shell.execute_reply.started":"2024-10-24T15:41:40.651145Z","shell.execute_reply":"2024-10-24T15:41:53.678928Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.34.2)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.25.1)\nCollecting peft\n  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.13.2\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(\"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\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T15:42:02.082598Z","iopub.execute_input":"2024-10-24T15:42:02.083355Z","iopub.status.idle":"2024-10-24T15:42:02.655562Z","shell.execute_reply.started":"2024-10-24T15:42:02.083311Z","shell.execute_reply":"2024-10-24T15:42:02.654631Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"meta-llama/Llama-3.2-1B\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T15:42:05.415080Z","iopub.execute_input":"2024-10-24T15:42:05.415481Z","iopub.status.idle":"2024-10-24T15:43:09.985441Z","shell.execute_reply.started":"2024-10-24T15:42:05.415435Z","shell.execute_reply":"2024-10-24T15:43:09.984520Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dbd0d5b9138485392bba963dc0e8d55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8eef4ca2313b44fc929fab56a8dfea17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7cc08afa62d43b29086c2a956cb4105"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e2e86d18dfe4c7b83ce9b352d144811"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56ac0fae5a74423fb79bfe572f5b091e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91926fdedf624e098213814e9d83b193"}},"metadata":{}}]},{"cell_type":"code","source":"from datasets import load_dataset\n\ndata = load_dataset(\"openai/gsm8k\", \"main\")\ndata","metadata":{"execution":{"iopub.status.busy":"2024-10-24T15:43:20.543804Z","iopub.execute_input":"2024-10-24T15:43:20.544846Z","iopub.status.idle":"2024-10-24T15:43:24.612244Z","shell.execute_reply.started":"2024-10-24T15:43:20.544803Z","shell.execute_reply":"2024-10-24T15:43:24.611140Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"625274baed6f4d7a844cc8237f80722b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10ecdb8efd22428b8687df3cff8e2672"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce88982116b94b9a871786d27fad8670"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd043dc2897441e9b3521acbd35f4e8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b8423092bfb4cc3be82958ec3bc62a0"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['question', 'answer'],\n        num_rows: 7473\n    })\n    test: Dataset({\n        features: ['question', 'answer'],\n        num_rows: 1319\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"def print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    return (f\"Trainable model parameters: {trainable_model_params}, All model parameters: {all_model_params},percentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\")\n\n\nprint(print_number_of_trainable_model_parameters(model))","metadata":{"execution":{"iopub.status.busy":"2024-10-24T15:43:29.338904Z","iopub.execute_input":"2024-10-24T15:43:29.339470Z","iopub.status.idle":"2024-10-24T15:43:29.346888Z","shell.execute_reply.started":"2024-10-24T15:43:29.339433Z","shell.execute_reply":"2024-10-24T15:43:29.345978Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Trainable model parameters: 1235814400, All model parameters: 1235814400,percentage of trainable model parameters: 100.00%\n","output_type":"stream"}]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, TaskType\n\nlora_config = LoraConfig(r=32,lora_alpha = 64, target_modules=[\"self_attn.q_proj\", \"self_attn.v_proj\"],\n                         lora_dropout = 0.2, bias =\"none\")\npeft_model_train = get_peft_model(model, lora_config)\nprint_number_of_trainable_model_parameters(peft_model_train)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T15:43:33.299660Z","iopub.execute_input":"2024-10-24T15:43:33.300067Z","iopub.status.idle":"2024-10-24T15:43:33.621346Z","shell.execute_reply.started":"2024-10-24T15:43:33.300029Z","shell.execute_reply":"2024-10-24T15:43:33.620503Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'Trainable model parameters: 3407872, All model parameters: 1239222272,percentage of trainable model parameters: 0.28%'"},"metadata":{}}]},{"cell_type":"code","source":"for name, module in model.named_modules():\n    print(name)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T15:43:35.363705Z","iopub.execute_input":"2024-10-24T15:43:35.364160Z","iopub.status.idle":"2024-10-24T15:43:35.376017Z","shell.execute_reply.started":"2024-10-24T15:43:35.364122Z","shell.execute_reply":"2024-10-24T15:43:35.375079Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"\nmodel\nmodel.embed_tokens\nmodel.layers\nmodel.layers.0\nmodel.layers.0.self_attn\nmodel.layers.0.self_attn.q_proj\nmodel.layers.0.self_attn.q_proj.base_layer\nmodel.layers.0.self_attn.q_proj.lora_dropout\nmodel.layers.0.self_attn.q_proj.lora_dropout.default\nmodel.layers.0.self_attn.q_proj.lora_A\nmodel.layers.0.self_attn.q_proj.lora_A.default\nmodel.layers.0.self_attn.q_proj.lora_B\nmodel.layers.0.self_attn.q_proj.lora_B.default\nmodel.layers.0.self_attn.q_proj.lora_embedding_A\nmodel.layers.0.self_attn.q_proj.lora_embedding_B\nmodel.layers.0.self_attn.q_proj.lora_magnitude_vector\nmodel.layers.0.self_attn.k_proj\nmodel.layers.0.self_attn.v_proj\nmodel.layers.0.self_attn.v_proj.base_layer\nmodel.layers.0.self_attn.v_proj.lora_dropout\nmodel.layers.0.self_attn.v_proj.lora_dropout.default\nmodel.layers.0.self_attn.v_proj.lora_A\nmodel.layers.0.self_attn.v_proj.lora_A.default\nmodel.layers.0.self_attn.v_proj.lora_B\nmodel.layers.0.self_attn.v_proj.lora_B.default\nmodel.layers.0.self_attn.v_proj.lora_embedding_A\nmodel.layers.0.self_attn.v_proj.lora_embedding_B\nmodel.layers.0.self_attn.v_proj.lora_magnitude_vector\nmodel.layers.0.self_attn.o_proj\nmodel.layers.0.self_attn.rotary_emb\nmodel.layers.0.mlp\nmodel.layers.0.mlp.gate_proj\nmodel.layers.0.mlp.up_proj\nmodel.layers.0.mlp.down_proj\nmodel.layers.0.mlp.act_fn\nmodel.layers.0.input_layernorm\nmodel.layers.0.post_attention_layernorm\nmodel.layers.1\nmodel.layers.1.self_attn\nmodel.layers.1.self_attn.q_proj\nmodel.layers.1.self_attn.q_proj.base_layer\nmodel.layers.1.self_attn.q_proj.lora_dropout\nmodel.layers.1.self_attn.q_proj.lora_dropout.default\nmodel.layers.1.self_attn.q_proj.lora_A\nmodel.layers.1.self_attn.q_proj.lora_A.default\nmodel.layers.1.self_attn.q_proj.lora_B\nmodel.layers.1.self_attn.q_proj.lora_B.default\nmodel.layers.1.self_attn.q_proj.lora_embedding_A\nmodel.layers.1.self_attn.q_proj.lora_embedding_B\nmodel.layers.1.self_attn.q_proj.lora_magnitude_vector\nmodel.layers.1.self_attn.k_proj\nmodel.layers.1.self_attn.v_proj\nmodel.layers.1.self_attn.v_proj.base_layer\nmodel.layers.1.self_attn.v_proj.lora_dropout\nmodel.layers.1.self_attn.v_proj.lora_dropout.default\nmodel.layers.1.self_attn.v_proj.lora_A\nmodel.layers.1.self_attn.v_proj.lora_A.default\nmodel.layers.1.self_attn.v_proj.lora_B\nmodel.layers.1.self_attn.v_proj.lora_B.default\nmodel.layers.1.self_attn.v_proj.lora_embedding_A\nmodel.layers.1.self_attn.v_proj.lora_embedding_B\nmodel.layers.1.self_attn.v_proj.lora_magnitude_vector\nmodel.layers.1.self_attn.o_proj\nmodel.layers.1.self_attn.rotary_emb\nmodel.layers.1.mlp\nmodel.layers.1.mlp.gate_proj\nmodel.layers.1.mlp.up_proj\nmodel.layers.1.mlp.down_proj\nmodel.layers.1.mlp.act_fn\nmodel.layers.1.input_layernorm\nmodel.layers.1.post_attention_layernorm\nmodel.layers.2\nmodel.layers.2.self_attn\nmodel.layers.2.self_attn.q_proj\nmodel.layers.2.self_attn.q_proj.base_layer\nmodel.layers.2.self_attn.q_proj.lora_dropout\nmodel.layers.2.self_attn.q_proj.lora_dropout.default\nmodel.layers.2.self_attn.q_proj.lora_A\nmodel.layers.2.self_attn.q_proj.lora_A.default\nmodel.layers.2.self_attn.q_proj.lora_B\nmodel.layers.2.self_attn.q_proj.lora_B.default\nmodel.layers.2.self_attn.q_proj.lora_embedding_A\nmodel.layers.2.self_attn.q_proj.lora_embedding_B\nmodel.layers.2.self_attn.q_proj.lora_magnitude_vector\nmodel.layers.2.self_attn.k_proj\nmodel.layers.2.self_attn.v_proj\nmodel.layers.2.self_attn.v_proj.base_layer\nmodel.layers.2.self_attn.v_proj.lora_dropout\nmodel.layers.2.self_attn.v_proj.lora_dropout.default\nmodel.layers.2.self_attn.v_proj.lora_A\nmodel.layers.2.self_attn.v_proj.lora_A.default\nmodel.layers.2.self_attn.v_proj.lora_B\nmodel.layers.2.self_attn.v_proj.lora_B.default\nmodel.layers.2.self_attn.v_proj.lora_embedding_A\nmodel.layers.2.self_attn.v_proj.lora_embedding_B\nmodel.layers.2.self_attn.v_proj.lora_magnitude_vector\nmodel.layers.2.self_attn.o_proj\nmodel.layers.2.self_attn.rotary_emb\nmodel.layers.2.mlp\nmodel.layers.2.mlp.gate_proj\nmodel.layers.2.mlp.up_proj\nmodel.layers.2.mlp.down_proj\nmodel.layers.2.mlp.act_fn\nmodel.layers.2.input_layernorm\nmodel.layers.2.post_attention_layernorm\nmodel.layers.3\nmodel.layers.3.self_attn\nmodel.layers.3.self_attn.q_proj\nmodel.layers.3.self_attn.q_proj.base_layer\nmodel.layers.3.self_attn.q_proj.lora_dropout\nmodel.layers.3.self_attn.q_proj.lora_dropout.default\nmodel.layers.3.self_attn.q_proj.lora_A\nmodel.layers.3.self_attn.q_proj.lora_A.default\nmodel.layers.3.self_attn.q_proj.lora_B\nmodel.layers.3.self_attn.q_proj.lora_B.default\nmodel.layers.3.self_attn.q_proj.lora_embedding_A\nmodel.layers.3.self_attn.q_proj.lora_embedding_B\nmodel.layers.3.self_attn.q_proj.lora_magnitude_vector\nmodel.layers.3.self_attn.k_proj\nmodel.layers.3.self_attn.v_proj\nmodel.layers.3.self_attn.v_proj.base_layer\nmodel.layers.3.self_attn.v_proj.lora_dropout\nmodel.layers.3.self_attn.v_proj.lora_dropout.default\nmodel.layers.3.self_attn.v_proj.lora_A\nmodel.layers.3.self_attn.v_proj.lora_A.default\nmodel.layers.3.self_attn.v_proj.lora_B\nmodel.layers.3.self_attn.v_proj.lora_B.default\nmodel.layers.3.self_attn.v_proj.lora_embedding_A\nmodel.layers.3.self_attn.v_proj.lora_embedding_B\nmodel.layers.3.self_attn.v_proj.lora_magnitude_vector\nmodel.layers.3.self_attn.o_proj\nmodel.layers.3.self_attn.rotary_emb\nmodel.layers.3.mlp\nmodel.layers.3.mlp.gate_proj\nmodel.layers.3.mlp.up_proj\nmodel.layers.3.mlp.down_proj\nmodel.layers.3.mlp.act_fn\nmodel.layers.3.input_layernorm\nmodel.layers.3.post_attention_layernorm\nmodel.layers.4\nmodel.layers.4.self_attn\nmodel.layers.4.self_attn.q_proj\nmodel.layers.4.self_attn.q_proj.base_layer\nmodel.layers.4.self_attn.q_proj.lora_dropout\nmodel.layers.4.self_attn.q_proj.lora_dropout.default\nmodel.layers.4.self_attn.q_proj.lora_A\nmodel.layers.4.self_attn.q_proj.lora_A.default\nmodel.layers.4.self_attn.q_proj.lora_B\nmodel.layers.4.self_attn.q_proj.lora_B.default\nmodel.layers.4.self_attn.q_proj.lora_embedding_A\nmodel.layers.4.self_attn.q_proj.lora_embedding_B\nmodel.layers.4.self_attn.q_proj.lora_magnitude_vector\nmodel.layers.4.self_attn.k_proj\nmodel.layers.4.self_attn.v_proj\nmodel.layers.4.self_attn.v_proj.base_layer\nmodel.layers.4.self_attn.v_proj.lora_dropout\nmodel.layers.4.self_attn.v_proj.lora_dropout.default\nmodel.layers.4.self_attn.v_proj.lora_A\nmodel.layers.4.self_attn.v_proj.lora_A.default\nmodel.layers.4.self_attn.v_proj.lora_B\nmodel.layers.4.self_attn.v_proj.lora_B.default\nmodel.layers.4.self_attn.v_proj.lora_embedding_A\nmodel.layers.4.self_attn.v_proj.lora_embedding_B\nmodel.layers.4.self_attn.v_proj.lora_magnitude_vector\nmodel.layers.4.self_attn.o_proj\nmodel.layers.4.self_attn.rotary_emb\nmodel.layers.4.mlp\nmodel.layers.4.mlp.gate_proj\nmodel.layers.4.mlp.up_proj\nmodel.layers.4.mlp.down_proj\nmodel.layers.4.mlp.act_fn\nmodel.layers.4.input_layernorm\nmodel.layers.4.post_attention_layernorm\nmodel.layers.5\nmodel.layers.5.self_attn\nmodel.layers.5.self_attn.q_proj\nmodel.layers.5.self_attn.q_proj.base_layer\nmodel.layers.5.self_attn.q_proj.lora_dropout\nmodel.layers.5.self_attn.q_proj.lora_dropout.default\nmodel.layers.5.self_attn.q_proj.lora_A\nmodel.layers.5.self_attn.q_proj.lora_A.default\nmodel.layers.5.self_attn.q_proj.lora_B\nmodel.layers.5.self_attn.q_proj.lora_B.default\nmodel.layers.5.self_attn.q_proj.lora_embedding_A\nmodel.layers.5.self_attn.q_proj.lora_embedding_B\nmodel.layers.5.self_attn.q_proj.lora_magnitude_vector\nmodel.layers.5.self_attn.k_proj\nmodel.layers.5.self_attn.v_proj\nmodel.layers.5.self_attn.v_proj.base_layer\nmodel.layers.5.self_attn.v_proj.lora_dropout\nmodel.layers.5.self_attn.v_proj.lora_dropout.default\nmodel.layers.5.self_attn.v_proj.lora_A\nmodel.layers.5.self_attn.v_proj.lora_A.default\nmodel.layers.5.self_attn.v_proj.lora_B\nmodel.layers.5.self_attn.v_proj.lora_B.default\nmodel.layers.5.self_attn.v_proj.lora_embedding_A\nmodel.layers.5.self_attn.v_proj.lora_embedding_B\nmodel.layers.5.self_attn.v_proj.lora_magnitude_vector\nmodel.layers.5.self_attn.o_proj\nmodel.layers.5.self_attn.rotary_emb\nmodel.layers.5.mlp\nmodel.layers.5.mlp.gate_proj\nmodel.layers.5.mlp.up_proj\nmodel.layers.5.mlp.down_proj\nmodel.layers.5.mlp.act_fn\nmodel.layers.5.input_layernorm\nmodel.layers.5.post_attention_layernorm\nmodel.layers.6\nmodel.layers.6.self_attn\nmodel.layers.6.self_attn.q_proj\nmodel.layers.6.self_attn.q_proj.base_layer\nmodel.layers.6.self_attn.q_proj.lora_dropout\nmodel.layers.6.self_attn.q_proj.lora_dropout.default\nmodel.layers.6.self_attn.q_proj.lora_A\nmodel.layers.6.self_attn.q_proj.lora_A.default\nmodel.layers.6.self_attn.q_proj.lora_B\nmodel.layers.6.self_attn.q_proj.lora_B.default\nmodel.layers.6.self_attn.q_proj.lora_embedding_A\nmodel.layers.6.self_attn.q_proj.lora_embedding_B\nmodel.layers.6.self_attn.q_proj.lora_magnitude_vector\nmodel.layers.6.self_attn.k_proj\nmodel.layers.6.self_attn.v_proj\nmodel.layers.6.self_attn.v_proj.base_layer\nmodel.layers.6.self_attn.v_proj.lora_dropout\nmodel.layers.6.self_attn.v_proj.lora_dropout.default\nmodel.layers.6.self_attn.v_proj.lora_A\nmodel.layers.6.self_attn.v_proj.lora_A.default\nmodel.layers.6.self_attn.v_proj.lora_B\nmodel.layers.6.self_attn.v_proj.lora_B.default\nmodel.layers.6.self_attn.v_proj.lora_embedding_A\nmodel.layers.6.self_attn.v_proj.lora_embedding_B\nmodel.layers.6.self_attn.v_proj.lora_magnitude_vector\nmodel.layers.6.self_attn.o_proj\nmodel.layers.6.self_attn.rotary_emb\nmodel.layers.6.mlp\nmodel.layers.6.mlp.gate_proj\nmodel.layers.6.mlp.up_proj\nmodel.layers.6.mlp.down_proj\nmodel.layers.6.mlp.act_fn\nmodel.layers.6.input_layernorm\nmodel.layers.6.post_attention_layernorm\nmodel.layers.7\nmodel.layers.7.self_attn\nmodel.layers.7.self_attn.q_proj\nmodel.layers.7.self_attn.q_proj.base_layer\nmodel.layers.7.self_attn.q_proj.lora_dropout\nmodel.layers.7.self_attn.q_proj.lora_dropout.default\nmodel.layers.7.self_attn.q_proj.lora_A\nmodel.layers.7.self_attn.q_proj.lora_A.default\nmodel.layers.7.self_attn.q_proj.lora_B\nmodel.layers.7.self_attn.q_proj.lora_B.default\nmodel.layers.7.self_attn.q_proj.lora_embedding_A\nmodel.layers.7.self_attn.q_proj.lora_embedding_B\nmodel.layers.7.self_attn.q_proj.lora_magnitude_vector\nmodel.layers.7.self_attn.k_proj\nmodel.layers.7.self_attn.v_proj\nmodel.layers.7.self_attn.v_proj.base_layer\nmodel.layers.7.self_attn.v_proj.lora_dropout\nmodel.layers.7.self_attn.v_proj.lora_dropout.default\nmodel.layers.7.self_attn.v_proj.lora_A\nmodel.layers.7.self_attn.v_proj.lora_A.default\nmodel.layers.7.self_attn.v_proj.lora_B\nmodel.layers.7.self_attn.v_proj.lora_B.default\nmodel.layers.7.self_attn.v_proj.lora_embedding_A\nmodel.layers.7.self_attn.v_proj.lora_embedding_B\nmodel.layers.7.self_attn.v_proj.lora_magnitude_vector\nmodel.layers.7.self_attn.o_proj\nmodel.layers.7.self_attn.rotary_emb\nmodel.layers.7.mlp\nmodel.layers.7.mlp.gate_proj\nmodel.layers.7.mlp.up_proj\nmodel.layers.7.mlp.down_proj\nmodel.layers.7.mlp.act_fn\nmodel.layers.7.input_layernorm\nmodel.layers.7.post_attention_layernorm\nmodel.layers.8\nmodel.layers.8.self_attn\nmodel.layers.8.self_attn.q_proj\nmodel.layers.8.self_attn.q_proj.base_layer\nmodel.layers.8.self_attn.q_proj.lora_dropout\nmodel.layers.8.self_attn.q_proj.lora_dropout.default\nmodel.layers.8.self_attn.q_proj.lora_A\nmodel.layers.8.self_attn.q_proj.lora_A.default\nmodel.layers.8.self_attn.q_proj.lora_B\nmodel.layers.8.self_attn.q_proj.lora_B.default\nmodel.layers.8.self_attn.q_proj.lora_embedding_A\nmodel.layers.8.self_attn.q_proj.lora_embedding_B\nmodel.layers.8.self_attn.q_proj.lora_magnitude_vector\nmodel.layers.8.self_attn.k_proj\nmodel.layers.8.self_attn.v_proj\nmodel.layers.8.self_attn.v_proj.base_layer\nmodel.layers.8.self_attn.v_proj.lora_dropout\nmodel.layers.8.self_attn.v_proj.lora_dropout.default\nmodel.layers.8.self_attn.v_proj.lora_A\nmodel.layers.8.self_attn.v_proj.lora_A.default\nmodel.layers.8.self_attn.v_proj.lora_B\nmodel.layers.8.self_attn.v_proj.lora_B.default\nmodel.layers.8.self_attn.v_proj.lora_embedding_A\nmodel.layers.8.self_attn.v_proj.lora_embedding_B\nmodel.layers.8.self_attn.v_proj.lora_magnitude_vector\nmodel.layers.8.self_attn.o_proj\nmodel.layers.8.self_attn.rotary_emb\nmodel.layers.8.mlp\nmodel.layers.8.mlp.gate_proj\nmodel.layers.8.mlp.up_proj\nmodel.layers.8.mlp.down_proj\nmodel.layers.8.mlp.act_fn\nmodel.layers.8.input_layernorm\nmodel.layers.8.post_attention_layernorm\nmodel.layers.9\nmodel.layers.9.self_attn\nmodel.layers.9.self_attn.q_proj\nmodel.layers.9.self_attn.q_proj.base_layer\nmodel.layers.9.self_attn.q_proj.lora_dropout\nmodel.layers.9.self_attn.q_proj.lora_dropout.default\nmodel.layers.9.self_attn.q_proj.lora_A\nmodel.layers.9.self_attn.q_proj.lora_A.default\nmodel.layers.9.self_attn.q_proj.lora_B\nmodel.layers.9.self_attn.q_proj.lora_B.default\nmodel.layers.9.self_attn.q_proj.lora_embedding_A\nmodel.layers.9.self_attn.q_proj.lora_embedding_B\nmodel.layers.9.self_attn.q_proj.lora_magnitude_vector\nmodel.layers.9.self_attn.k_proj\nmodel.layers.9.self_attn.v_proj\nmodel.layers.9.self_attn.v_proj.base_layer\nmodel.layers.9.self_attn.v_proj.lora_dropout\nmodel.layers.9.self_attn.v_proj.lora_dropout.default\nmodel.layers.9.self_attn.v_proj.lora_A\nmodel.layers.9.self_attn.v_proj.lora_A.default\nmodel.layers.9.self_attn.v_proj.lora_B\nmodel.layers.9.self_attn.v_proj.lora_B.default\nmodel.layers.9.self_attn.v_proj.lora_embedding_A\nmodel.layers.9.self_attn.v_proj.lora_embedding_B\nmodel.layers.9.self_attn.v_proj.lora_magnitude_vector\nmodel.layers.9.self_attn.o_proj\nmodel.layers.9.self_attn.rotary_emb\nmodel.layers.9.mlp\nmodel.layers.9.mlp.gate_proj\nmodel.layers.9.mlp.up_proj\nmodel.layers.9.mlp.down_proj\nmodel.layers.9.mlp.act_fn\nmodel.layers.9.input_layernorm\nmodel.layers.9.post_attention_layernorm\nmodel.layers.10\nmodel.layers.10.self_attn\nmodel.layers.10.self_attn.q_proj\nmodel.layers.10.self_attn.q_proj.base_layer\nmodel.layers.10.self_attn.q_proj.lora_dropout\nmodel.layers.10.self_attn.q_proj.lora_dropout.default\nmodel.layers.10.self_attn.q_proj.lora_A\nmodel.layers.10.self_attn.q_proj.lora_A.default\nmodel.layers.10.self_attn.q_proj.lora_B\nmodel.layers.10.self_attn.q_proj.lora_B.default\nmodel.layers.10.self_attn.q_proj.lora_embedding_A\nmodel.layers.10.self_attn.q_proj.lora_embedding_B\nmodel.layers.10.self_attn.q_proj.lora_magnitude_vector\nmodel.layers.10.self_attn.k_proj\nmodel.layers.10.self_attn.v_proj\nmodel.layers.10.self_attn.v_proj.base_layer\nmodel.layers.10.self_attn.v_proj.lora_dropout\nmodel.layers.10.self_attn.v_proj.lora_dropout.default\nmodel.layers.10.self_attn.v_proj.lora_A\nmodel.layers.10.self_attn.v_proj.lora_A.default\nmodel.layers.10.self_attn.v_proj.lora_B\nmodel.layers.10.self_attn.v_proj.lora_B.default\nmodel.layers.10.self_attn.v_proj.lora_embedding_A\nmodel.layers.10.self_attn.v_proj.lora_embedding_B\nmodel.layers.10.self_attn.v_proj.lora_magnitude_vector\nmodel.layers.10.self_attn.o_proj\nmodel.layers.10.self_attn.rotary_emb\nmodel.layers.10.mlp\nmodel.layers.10.mlp.gate_proj\nmodel.layers.10.mlp.up_proj\nmodel.layers.10.mlp.down_proj\nmodel.layers.10.mlp.act_fn\nmodel.layers.10.input_layernorm\nmodel.layers.10.post_attention_layernorm\nmodel.layers.11\nmodel.layers.11.self_attn\nmodel.layers.11.self_attn.q_proj\nmodel.layers.11.self_attn.q_proj.base_layer\nmodel.layers.11.self_attn.q_proj.lora_dropout\nmodel.layers.11.self_attn.q_proj.lora_dropout.default\nmodel.layers.11.self_attn.q_proj.lora_A\nmodel.layers.11.self_attn.q_proj.lora_A.default\nmodel.layers.11.self_attn.q_proj.lora_B\nmodel.layers.11.self_attn.q_proj.lora_B.default\nmodel.layers.11.self_attn.q_proj.lora_embedding_A\nmodel.layers.11.self_attn.q_proj.lora_embedding_B\nmodel.layers.11.self_attn.q_proj.lora_magnitude_vector\nmodel.layers.11.self_attn.k_proj\nmodel.layers.11.self_attn.v_proj\nmodel.layers.11.self_attn.v_proj.base_layer\nmodel.layers.11.self_attn.v_proj.lora_dropout\nmodel.layers.11.self_attn.v_proj.lora_dropout.default\nmodel.layers.11.self_attn.v_proj.lora_A\nmodel.layers.11.self_attn.v_proj.lora_A.default\nmodel.layers.11.self_attn.v_proj.lora_B\nmodel.layers.11.self_attn.v_proj.lora_B.default\nmodel.layers.11.self_attn.v_proj.lora_embedding_A\nmodel.layers.11.self_attn.v_proj.lora_embedding_B\nmodel.layers.11.self_attn.v_proj.lora_magnitude_vector\nmodel.layers.11.self_attn.o_proj\nmodel.layers.11.self_attn.rotary_emb\nmodel.layers.11.mlp\nmodel.layers.11.mlp.gate_proj\nmodel.layers.11.mlp.up_proj\nmodel.layers.11.mlp.down_proj\nmodel.layers.11.mlp.act_fn\nmodel.layers.11.input_layernorm\nmodel.layers.11.post_attention_layernorm\nmodel.layers.12\nmodel.layers.12.self_attn\nmodel.layers.12.self_attn.q_proj\nmodel.layers.12.self_attn.q_proj.base_layer\nmodel.layers.12.self_attn.q_proj.lora_dropout\nmodel.layers.12.self_attn.q_proj.lora_dropout.default\nmodel.layers.12.self_attn.q_proj.lora_A\nmodel.layers.12.self_attn.q_proj.lora_A.default\nmodel.layers.12.self_attn.q_proj.lora_B\nmodel.layers.12.self_attn.q_proj.lora_B.default\nmodel.layers.12.self_attn.q_proj.lora_embedding_A\nmodel.layers.12.self_attn.q_proj.lora_embedding_B\nmodel.layers.12.self_attn.q_proj.lora_magnitude_vector\nmodel.layers.12.self_attn.k_proj\nmodel.layers.12.self_attn.v_proj\nmodel.layers.12.self_attn.v_proj.base_layer\nmodel.layers.12.self_attn.v_proj.lora_dropout\nmodel.layers.12.self_attn.v_proj.lora_dropout.default\nmodel.layers.12.self_attn.v_proj.lora_A\nmodel.layers.12.self_attn.v_proj.lora_A.default\nmodel.layers.12.self_attn.v_proj.lora_B\nmodel.layers.12.self_attn.v_proj.lora_B.default\nmodel.layers.12.self_attn.v_proj.lora_embedding_A\nmodel.layers.12.self_attn.v_proj.lora_embedding_B\nmodel.layers.12.self_attn.v_proj.lora_magnitude_vector\nmodel.layers.12.self_attn.o_proj\nmodel.layers.12.self_attn.rotary_emb\nmodel.layers.12.mlp\nmodel.layers.12.mlp.gate_proj\nmodel.layers.12.mlp.up_proj\nmodel.layers.12.mlp.down_proj\nmodel.layers.12.mlp.act_fn\nmodel.layers.12.input_layernorm\nmodel.layers.12.post_attention_layernorm\nmodel.layers.13\nmodel.layers.13.self_attn\nmodel.layers.13.self_attn.q_proj\nmodel.layers.13.self_attn.q_proj.base_layer\nmodel.layers.13.self_attn.q_proj.lora_dropout\nmodel.layers.13.self_attn.q_proj.lora_dropout.default\nmodel.layers.13.self_attn.q_proj.lora_A\nmodel.layers.13.self_attn.q_proj.lora_A.default\nmodel.layers.13.self_attn.q_proj.lora_B\nmodel.layers.13.self_attn.q_proj.lora_B.default\nmodel.layers.13.self_attn.q_proj.lora_embedding_A\nmodel.layers.13.self_attn.q_proj.lora_embedding_B\nmodel.layers.13.self_attn.q_proj.lora_magnitude_vector\nmodel.layers.13.self_attn.k_proj\nmodel.layers.13.self_attn.v_proj\nmodel.layers.13.self_attn.v_proj.base_layer\nmodel.layers.13.self_attn.v_proj.lora_dropout\nmodel.layers.13.self_attn.v_proj.lora_dropout.default\nmodel.layers.13.self_attn.v_proj.lora_A\nmodel.layers.13.self_attn.v_proj.lora_A.default\nmodel.layers.13.self_attn.v_proj.lora_B\nmodel.layers.13.self_attn.v_proj.lora_B.default\nmodel.layers.13.self_attn.v_proj.lora_embedding_A\nmodel.layers.13.self_attn.v_proj.lora_embedding_B\nmodel.layers.13.self_attn.v_proj.lora_magnitude_vector\nmodel.layers.13.self_attn.o_proj\nmodel.layers.13.self_attn.rotary_emb\nmodel.layers.13.mlp\nmodel.layers.13.mlp.gate_proj\nmodel.layers.13.mlp.up_proj\nmodel.layers.13.mlp.down_proj\nmodel.layers.13.mlp.act_fn\nmodel.layers.13.input_layernorm\nmodel.layers.13.post_attention_layernorm\nmodel.layers.14\nmodel.layers.14.self_attn\nmodel.layers.14.self_attn.q_proj\nmodel.layers.14.self_attn.q_proj.base_layer\nmodel.layers.14.self_attn.q_proj.lora_dropout\nmodel.layers.14.self_attn.q_proj.lora_dropout.default\nmodel.layers.14.self_attn.q_proj.lora_A\nmodel.layers.14.self_attn.q_proj.lora_A.default\nmodel.layers.14.self_attn.q_proj.lora_B\nmodel.layers.14.self_attn.q_proj.lora_B.default\nmodel.layers.14.self_attn.q_proj.lora_embedding_A\nmodel.layers.14.self_attn.q_proj.lora_embedding_B\nmodel.layers.14.self_attn.q_proj.lora_magnitude_vector\nmodel.layers.14.self_attn.k_proj\nmodel.layers.14.self_attn.v_proj\nmodel.layers.14.self_attn.v_proj.base_layer\nmodel.layers.14.self_attn.v_proj.lora_dropout\nmodel.layers.14.self_attn.v_proj.lora_dropout.default\nmodel.layers.14.self_attn.v_proj.lora_A\nmodel.layers.14.self_attn.v_proj.lora_A.default\nmodel.layers.14.self_attn.v_proj.lora_B\nmodel.layers.14.self_attn.v_proj.lora_B.default\nmodel.layers.14.self_attn.v_proj.lora_embedding_A\nmodel.layers.14.self_attn.v_proj.lora_embedding_B\nmodel.layers.14.self_attn.v_proj.lora_magnitude_vector\nmodel.layers.14.self_attn.o_proj\nmodel.layers.14.self_attn.rotary_emb\nmodel.layers.14.mlp\nmodel.layers.14.mlp.gate_proj\nmodel.layers.14.mlp.up_proj\nmodel.layers.14.mlp.down_proj\nmodel.layers.14.mlp.act_fn\nmodel.layers.14.input_layernorm\nmodel.layers.14.post_attention_layernorm\nmodel.layers.15\nmodel.layers.15.self_attn\nmodel.layers.15.self_attn.q_proj\nmodel.layers.15.self_attn.q_proj.base_layer\nmodel.layers.15.self_attn.q_proj.lora_dropout\nmodel.layers.15.self_attn.q_proj.lora_dropout.default\nmodel.layers.15.self_attn.q_proj.lora_A\nmodel.layers.15.self_attn.q_proj.lora_A.default\nmodel.layers.15.self_attn.q_proj.lora_B\nmodel.layers.15.self_attn.q_proj.lora_B.default\nmodel.layers.15.self_attn.q_proj.lora_embedding_A\nmodel.layers.15.self_attn.q_proj.lora_embedding_B\nmodel.layers.15.self_attn.q_proj.lora_magnitude_vector\nmodel.layers.15.self_attn.k_proj\nmodel.layers.15.self_attn.v_proj\nmodel.layers.15.self_attn.v_proj.base_layer\nmodel.layers.15.self_attn.v_proj.lora_dropout\nmodel.layers.15.self_attn.v_proj.lora_dropout.default\nmodel.layers.15.self_attn.v_proj.lora_A\nmodel.layers.15.self_attn.v_proj.lora_A.default\nmodel.layers.15.self_attn.v_proj.lora_B\nmodel.layers.15.self_attn.v_proj.lora_B.default\nmodel.layers.15.self_attn.v_proj.lora_embedding_A\nmodel.layers.15.self_attn.v_proj.lora_embedding_B\nmodel.layers.15.self_attn.v_proj.lora_magnitude_vector\nmodel.layers.15.self_attn.o_proj\nmodel.layers.15.self_attn.rotary_emb\nmodel.layers.15.mlp\nmodel.layers.15.mlp.gate_proj\nmodel.layers.15.mlp.up_proj\nmodel.layers.15.mlp.down_proj\nmodel.layers.15.mlp.act_fn\nmodel.layers.15.input_layernorm\nmodel.layers.15.post_attention_layernorm\nmodel.norm\nmodel.rotary_emb\nlm_head\n","output_type":"stream"}]},{"cell_type":"code","source":"def preprocess_function(examples):\n    inputs = [q + \" \" + a for q, a in zip(examples['question'], examples['answer'])]\n    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n    \n    return model_inputs\ntokenized_datasets = data.map(preprocess_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T15:43:40.623440Z","iopub.execute_input":"2024-10-24T15:43:40.623861Z","iopub.status.idle":"2024-10-24T15:43:45.872583Z","shell.execute_reply.started":"2024-10-24T15:43:40.623819Z","shell.execute_reply":"2024-10-24T15:43:45.871577Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7473 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f20e960654af48dca76c3e0f4cdc3658"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1319 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"463f605d2b9d41c2a3d3a237e0399114"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/results\",\n    num_train_epochs=2,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir=\"/kaggle/working/logs\",\n)\n\ntrainer = Trainer(\n    model=peft_model_train,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T15:43:56.083138Z","iopub.execute_input":"2024-10-24T15:43:56.083907Z","iopub.status.idle":"2024-10-24T15:50:00.846935Z","shell.execute_reply.started":"2024-10-24T15:43:56.083864Z","shell.execute_reply":"2024-10-24T15:50:00.845388Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011114338622222577, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f892c547934c4c448de4b0d3cefff7ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241024_154425-mil7ifqb</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/avg16-iit-roorkee/huggingface/runs/mil7ifqb' target=\"_blank\">/kaggle/working/results</a></strong> to <a href='https://wandb.ai/avg16-iit-roorkee/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/avg16-iit-roorkee/huggingface' target=\"_blank\">https://wandb.ai/avg16-iit-roorkee/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/avg16-iit-roorkee/huggingface/runs/mil7ifqb' target=\"_blank\">https://wandb.ai/avg16-iit-roorkee/huggingface/runs/mil7ifqb</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='392' max='7474' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 392/7474 05:28 < 1:39:33, 1.19 it/s, Epoch 0.10/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 20\u001b[0m\n\u001b[1;32m      3\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      4\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/working/results\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     logging_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/working/logs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     14\u001b[0m     model\u001b[38;5;241m=\u001b[39mpeft_model_train,\n\u001b[1;32m     15\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     16\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     17\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     18\u001b[0m )\n\u001b[0;32m---> 20\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2388\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2394\u001b[0m ):\n\u001b[1;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3518\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3516\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3518\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:2196\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2196\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"peft_model_path=\"/kaggle/working/pre\"\n\ntrainer.model.save_pretrained(peft_model_path)\ntokenizer.save_pretrained(peft_model_path)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T14:35:16.832888Z","iopub.execute_input":"2024-10-24T14:35:16.833562Z","iopub.status.idle":"2024-10-24T14:35:17.402269Z","shell.execute_reply.started":"2024-10-24T14:35:16.833520Z","shell.execute_reply":"2024-10-24T14:35:17.399301Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/pre/tokenizer_config.json',\n '/kaggle/working/pre/special_tokens_map.json',\n '/kaggle/working/pre/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}